Some introduction containing historical information and stuff, and motivation for this section.

\section{Restricted Boltzmann Machine}\label{sec:rbm}

Restricted Boltzmann machine is a energy-based generative model which include hidden and visible variables (ref to Metha). It consists of a two-layer network with a two dimensional matrix $W_{ij}$ telling how strong the connections between the hidden and visible nodes are. The energy related to a configuration of the nodes is what forms the basis of our model. It is given in \eqref{eq:E_rbm}.

\begin{equation}\label{eq:E_rbm}
E(\mathbf{v}, \mathbf{h}) = - \sum_{i} a_i(v_i) - \sum_{\mu} b_j(h_j) - \sum_{ij} W_{ij}v_i h_j 
\end{equation}

Here $a_i(v_i)$ and $b_j(h_j)$ are bias functions of the visible and hidden layers which we are allowed to choose our self. In our case we want the visible layer to take a continuous form and the hidden layer to be binary, (Gaussian-binary), meaning $a_i$ and $b_j$ takes the following from

$$a_i(v_i) = \frac{v_i^2}{2 \sigma_i^2}, \ \ \ b_j(h_j)= b_jh_j.$$

We are working with restricted Boltzmann machine meaning there is no connection between nodes within layers, only between layers. Also this network is a generative one so we want our network to learn a probability distribution. We begin with the joint probability distribution of the visible and hidden nodes is given in \eqref{eq:F_rbm_joint} where $Z$ is the partition function, see \eqref{eq:Z}. 

\begin{equation}\label{eq:F_rbm_joint}
F_{rbm} (\mathbf{X}, \mathbf{h}) = \frac{1}{Z} e^{-E(\mathbf{X}, \mathbf{h})}
\end{equation}

\begin{equation}\label{eq:Z}
Z = \int \int \frac{1}{Z} e^{-E(\mathbf{X}, \mathbf{h})} d\mathbf{x}d\mathbf{h}
\end{equation}

From \eqref{eq:F_rbm_joint} we can marginalize over all the hidden units and get the distribution over the visible units. And as mentioned above this is what we use to represent our wave function. 

\begin{align}\label{eq:F_rbm_marg}
F_{rbm} (\mathbf{X}) &= \sum_{\mathbf{h}} F_{rbm} (\mathbf{X}, \mathbf{h}) \\
&= \frac{1}{Z} \sum_{\mathbf{h}} e^{-E(\mathbf{X}, \mathbf{h})}
\end{align}

Since we have no data to train we use the technique of reinforcement learning. We feed the network with input using Monte Carlo method and based on the variational principle we find the ground state of the system by seeking the configuration that gives the lowest quantum mechanical energy. According to this principle we change the weights and biases by gradient descent method and hopefully the network will converge towards the correct state.  

\section{Gibbs Sampling}

We represent the wave equation on a new form. This requires that the wave function is positive definite. 

\begin{equation}\label{eq:Gibbs}
\psi(x) = \sqrt{(F_{rbm})}
\end{equation}

Rather than on the form we have used before.

\begin{equation}\label{eq:Gibbs_new}
\psi(x) = (F_{rbm})
\end{equation}